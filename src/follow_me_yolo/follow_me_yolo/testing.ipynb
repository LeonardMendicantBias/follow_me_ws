{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f4a3db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e7b750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics 8.3.130 üöÄ Python-3.10.12 torch-2.3.0 CUDA:0 (Orin, 7620MiB)\n",
      "YOLO11n-pose summary (fused): 109 layers, 2,866,468 parameters, 0 gradients, 7.4 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n-pose.pt' with input shape (1, 3, 256, 448) BCHW and output shape(s) (1, 56, 2352) (6.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.52...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 4.5s, saved as 'yolo11n-pose.onnx' (11.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 8.6.2...\n",
      "[05/12/2025-20:59:45] [TRT] [I] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 705, GPU 6299 (MiB)\n",
      "[05/12/2025-20:59:52] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1154, GPU +687, now: CPU 1895, GPU 7010 (MiB)\n",
      "[05/12/2025-20:59:53] [TRT] [I] ----------------------------------------------------------------\n",
      "[05/12/2025-20:59:53] [TRT] [I] Input filename:   yolo11n-pose.onnx\n",
      "[05/12/2025-20:59:53] [TRT] [I] ONNX IR version:  0.0.8\n",
      "[05/12/2025-20:59:53] [TRT] [I] Opset version:    17\n",
      "[05/12/2025-20:59:53] [TRT] [I] Producer name:    pytorch\n",
      "[05/12/2025-20:59:53] [TRT] [I] Producer version: 2.3.0\n",
      "[05/12/2025-20:59:53] [TRT] [I] Domain:           \n",
      "[05/12/2025-20:59:53] [TRT] [I] Model version:    0\n",
      "[05/12/2025-20:59:53] [TRT] [I] Doc string:       \n",
      "[05/12/2025-20:59:53] [TRT] [I] ----------------------------------------------------------------\n",
      "[05/12/2025-20:59:53] [TRT] [W] onnx2trt_utils.cpp:372: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 256, 448) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 56, 2352) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as yolo11n-pose.engine\n",
      "[05/12/2025-20:59:53] [TRT] [I] Graph optimization time: 0.147397 seconds.\n",
      "[05/12/2025-20:59:53] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[05/12/2025-21:16:29] [TRT] [I] Detected 1 inputs and 4 output network tensors.\n",
      "[05/12/2025-21:16:30] [TRT] [I] Total Host Persistent Memory: 526000\n",
      "[05/12/2025-21:16:30] [TRT] [I] Total Device Persistent Memory: 3072\n",
      "[05/12/2025-21:16:30] [TRT] [I] Total Scratch Memory: 0\n",
      "[05/12/2025-21:16:30] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 7 MiB, GPU 37 MiB\n",
      "[05/12/2025-21:16:30] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 189 steps to complete.\n",
      "[05/12/2025-21:16:30] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 29.4639ms to assign 10 blocks to 189 nodes requiring 2907648 bytes.\n",
      "[05/12/2025-21:16:30] [TRT] [I] Total Activation Memory: 2906624\n",
      "[05/12/2025-21:16:31] [TRT] [W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[05/12/2025-21:16:31] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[05/12/2025-21:16:31] [TRT] [W] Check verbose logs for the list of affected weights.\n",
      "[05/12/2025-21:16:31] [TRT] [W] - 74 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[05/12/2025-21:16:31] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +5, GPU +8, now: CPU 5, GPU 8 (MiB)\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ‚úÖ 1011.8s, saved as 'yolo11n-pose.engine' (8.6 MB)\n",
      "\n",
      "Export complete (1014.0s)\n",
      "Results saved to \u001b[1m/home/nardy/follow_me_ws/src/follow_me_yolo/follow_me_yolo\u001b[0m\n",
      "Predict:         yolo predict task=pose model=yolo11n-pose.engine imgsz=256,448 half \n",
      "Validate:        yolo val task=pose model=yolo11n-pose.engine imgsz=256,448 data=/ultralytics/ultralytics/cfg/datasets/coco-pose.yaml half WARNING ‚ö†Ô∏è non-PyTorch val requires square images, 'imgsz=[256, 448]' will not work. Use export 'imgsz=448' if val is required.\n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolo11n-pose.pt\")\n",
    "# model.export(\n",
    "#     format=\"engine\",\n",
    "#     imgsz=[256, 448],\n",
    "#     # dynamic=True,\n",
    "#     # simplify=True,\n",
    "#     half=True,\n",
    "#     # int8=True,\n",
    "# )\n",
    "# tensorrt_model = YOLO(\"yolo11n-pose.engine\")\n",
    "\n",
    "# self.model = YOLO(\"yolo11n-pose.pt\")\n",
    "model.export(\n",
    "    format=\"engine\",\n",
    "    imgsz=[256, 448],\n",
    "    half=True,\n",
    ")\n",
    "tensorrt_model = YOLO(\"yolo11n-pose.engine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af7b161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"./bus.jpg\")\n",
    "img_ = cv2.resize(img, (448, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29dcde8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 448, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbe5289f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.9 ms ¬± 618 Œºs per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model(img_, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4378e7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.6 ms ¬± 155 Œºs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit tensorrt_model(img, imgsz=[256, 448], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac14747",
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton = [\n",
    "    [16, 14],\n",
    "    [14, 12],\n",
    "    [17, 15],\n",
    "    [15, 13],\n",
    "    [12, 13],\n",
    "    [6, 12],\n",
    "    [7, 13],\n",
    "    [6, 7],\n",
    "    [6, 8],\n",
    "    [7, 9],\n",
    "    [8, 10],\n",
    "    [9, 11],\n",
    "    [2, 3],\n",
    "    [1, 2],\n",
    "    [1, 3],\n",
    "    [2, 4],\n",
    "    [3, 5],\n",
    "    [4, 6],\n",
    "    [5, 7],\n",
    "]\n",
    "pose_palette = np.array([\n",
    "    [255, 128, 0],\n",
    "    [255, 153, 51],\n",
    "    [255, 178, 102],\n",
    "    [230, 230, 0],\n",
    "    [255, 153, 255],\n",
    "    [153, 204, 255],\n",
    "    [255, 102, 255],\n",
    "    [255, 51, 255],\n",
    "    [102, 178, 255],\n",
    "    [51, 153, 255],\n",
    "    [255, 153, 153],\n",
    "    [255, 102, 102],\n",
    "    [255, 51, 51],\n",
    "    [153, 255, 153],\n",
    "    [102, 255, 102],\n",
    "    [51, 255, 51],\n",
    "    [0, 255, 0],\n",
    "    [0, 0, 255],\n",
    "    [255, 0, 0],\n",
    "    [255, 255, 255],\n",
    "], dtype=np.uint8)\n",
    "\n",
    "limb_color = pose_palette[[9, 9, 9, 9, 7, 7, 7, 0, 0, 0, 0, 0, 16, 16, 16, 16, 16, 16, 16]]\n",
    "kpt_color = pose_palette[[16, 16, 16, 16, 16, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 9]]\n",
    "\n",
    "conf_thres = 0.5\n",
    "result = results[0]\n",
    "img = cv2.imread(\"./bus.jpg\")\n",
    "for bbox, kpts, confs in zip(result.boxes.xywh, result.keypoints.xy, result.keypoints.conf):\n",
    "    x, y, w, h = map(int, bbox)\n",
    "    cv2.rectangle(img, (x-w//2, y-h//2), (x+w//2, y+h//2), (0, 255, 0), 2)\n",
    "    \n",
    "    for j, (kpt, conf) in enumerate(zip(kpts, confs)):\n",
    "        if conf < conf_thres:\n",
    "            continue\n",
    "\n",
    "        x, y = int(kpt[0]), int(kpt[1])\n",
    "        cv2.circle(img, (x, y), 5, kpt_color[j].tolist(), -1)\n",
    "    \n",
    "    for i, sk in enumerate(skeleton):\n",
    "        pos1 = (int(kpts[(sk[0] - 1), 0]), int(kpts[(sk[0] - 1), 1]))\n",
    "        pos2 = (int(kpts[(sk[1] - 1), 0]), int(kpts[(sk[1] - 1), 1]))\n",
    "\n",
    "        conf1 = confs[(sk[0] - 1)]\n",
    "        conf2 = confs[(sk[1] - 1)]\n",
    "        if conf1 < conf_thres or conf2 < conf_thres:\n",
    "            continue\n",
    "        # if pos1[0] % shape[1] == 0 or pos1[1] % shape[0] == 0 or pos1[0] < 0 or pos1[1] < 0:\n",
    "        #     continue\n",
    "        # if pos2[0] % shape[1] == 0 or pos2[1] % shape[0] == 0 or pos2[0] < 0 or pos2[1] < 0:\n",
    "        #     continue\n",
    "        cv2.line(\n",
    "            img,\n",
    "            pos1, pos2,\n",
    "            limb_color[i].tolist(),\n",
    "            thickness=3,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba9ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c544e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].keypoints.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf512c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results[0].boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b5eafd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
